#!/usr/bin/env python3
"""
ETL Pipeline for Supply Constraint Metrics

This script demonstrates the complete ETL pipeline for processing supply constraint data
from multiple sources and calculating metrics for the Aker Property Model.
"""

import sys
import os
import argparse
import logging
from datetime import datetime, date
from typing import List, Optional

# Add the src directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

try:
    from aker_core.connectors.supply_data import SupplyDataETL
    from aker_core.supply import calculate_supply_metrics, get_supply_scores_for_scoring
    from aker_core.validation.supply_validation import run_supply_validation_cli
    from aker_core.monitoring.supply_monitoring import get_supply_monitor

    print("‚úÖ Successfully imported supply ETL components")

except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Make sure all dependencies are installed")
    sys.exit(1)


def setup_logging(log_level: str = "INFO", log_file: str = None):
    """Set up logging configuration."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            *(logging.FileHandler(log_file) if log_file else [])
        ]
    )


def run_supply_etl_pipeline(
    msa_ids: List[str],
    start_year: int = 2010,
    end_year: Optional[int] = None,
    validate_results: bool = True,
    dry_run: bool = False
) -> dict:
    """
    Run the complete supply ETL pipeline.

    Args:
        msa_ids: List of MSA identifiers to process
        start_year: Start year for data collection
        end_year: End year for data collection (default: current year)
        validate_results: Whether to run validation after processing
        dry_run: Whether to run in dry-run mode (no database writes)

    Returns:
        Pipeline execution results
    """
    print("üöÄ Starting Supply Constraint ETL Pipeline")
    print(f"   MSAs: {len(msa_ids)} markets")
    print(f"   Years: {start_year} - {end_year or datetime.now().year}")
    print(f"   Validation: {'Enabled' if validate_results else 'Disabled'}")
    print(f"   Dry Run: {'Yes' if dry_run else 'No'}")
    print()

    # Initialize ETL processor
    etl_processor = SupplyDataETL()

    # Step 1: Extract data from all sources
    print("üì• Step 1: Extracting data from all sources...")
    try:
        raw_data = etl_processor.extract_supply_data(msa_ids, start_year, end_year)
        print(f"   ‚úÖ Extracted data for {raw_data['msa_count']} MSAs")
        print(f"   üìä Data summary: {raw_data['data_years']}")
    except Exception as e:
        print(f"‚ùå Data extraction failed: {e}")
        return {"error": f"Extraction failed: {e}"}

    # Step 2: Transform data for calculations
    print("\nüîÑ Step 2: Transforming data for calculations...")
    try:
        transformed_data = etl_processor.transform_supply_data(raw_data)
        print("   ‚úÖ Data transformation completed")
        print(f"   üìà Quality assessment: {transformed_data['data_quality']}")
    except Exception as e:
        print(f"‚ùå Data transformation failed: {e}")
        return {"error": f"Transformation failed: {e}"}

    # Step 3: Load data to database (if not dry run)
    if not dry_run:
        print("\nüíæ Step 3: Loading data to database...")
        try:
            # In production, this would use a real database session
            # For now, simulate the loading process
            print("   üìã Processing each MSA...")

            for msa_id in transformed_data["permits_by_msa"]:
                print(f"   ‚Ä¢ Processing {msa_id}...")

                # Extract data for this MSA
                permits = list(transformed_data["permits_by_msa"][msa_id].values())
                households = list(transformed_data["households_by_msa"][msa_id].values())
                hud_data = transformed_data["vacancy_by_msa"].get(msa_id, [])
                lease_data = transformed_data["leases_by_msa"].get(msa_id, [])

                print(f"     Permits: {len(permits)} years")
                print(f"     Households: {len(households)} years")
                print(f"     Vacancy: {len(hud_data)} records")
                print(f"     Leases: {len(lease_data)} records")

                # Calculate metrics (without actual database session)
                try:
                    from aker_core.supply import elasticity, vacancy, leaseup_tom

                    elasticity_value = elasticity(permits, households, years=3)
                    vacancy_value = vacancy({'year': [h['year'] for h in hud_data],
                                           'vacancy_rate': [h['vacancy_rate'] for h in hud_data],
                                           'geography': [msa_id] * len(hud_data)})
                    leaseup_value = leaseup_tom({'lease_date': [l['lease_date'] for l in lease_data],
                                               'days_on_market': [l['days_on_market'] for l in lease_data]})

                    print(f"     ‚úÖ Calculated: Elasticity={elasticity_value".2f"}, Vacancy={vacancy_value".2f"}%, Lease-up={leaseup_value".1f"} days")

                except Exception as e:
                    print(f"     ‚ö†Ô∏è  Calculation warning: {e}")

            print("   ‚úÖ Data loading simulation completed")

        except Exception as e:
            print(f"‚ùå Data loading failed: {e}")
            return {"error": f"Loading failed: {e}"}
    else:
        print("\nüíæ Step 3: Database loading (SKIPPED - dry run)")

    # Step 4: Validate results
    if validate_results:
        print("\n‚úÖ Step 4: Validating results...")
        try:
            # In production, this would run Great Expectations validation
            print("   üìä Running data quality validation...")
            print("   ‚úÖ Validation completed (simulated)")
        except Exception as e:
            print(f"‚ö†Ô∏è  Validation warning: {e}")

    # Step 5: Generate summary report
    print("\nüìä Step 5: Generating pipeline summary...")

    total_msas = len(msa_ids)
    successful_msas = total_msas  # In simulation, assume all succeed

    summary = {
        "pipeline_status": "completed",
        "msas_processed": successful_msas,
        "msas_failed": total_msas - successful_msas,
        "data_years": raw_data['data_years'],
        "extraction_timestamp": raw_data['extraction_timestamp'],
        "transformation_timestamp": transformed_data['transformation_timestamp'],
        "dry_run": dry_run,
        "validation_enabled": validate_results,
        "completed_at": datetime.now().isoformat()
    }

    print(f"   üìà Summary: {successful_msas}/{total_msas} MSAs processed successfully")
    print(f"   ‚è±Ô∏è  Data span: {raw_data['data_years']}")
    print(f"   ‚úÖ Pipeline completed at {summary['completed_at']}")

    return summary


def run_production_etl_pipeline(
    msa_ids: List[str],
    data_vintage: str,
    batch_size: int = 10,
    max_workers: int = 4
) -> dict:
    """
    Run production ETL pipeline with parallel processing.

    Args:
        msa_ids: List of MSA identifiers
        data_vintage: Data vintage string
        batch_size: Batch size for processing
        max_workers: Maximum parallel workers

    Returns:
        Production pipeline results
    """
    print("üè≠ Running Production ETL Pipeline")
    print(f"   MSAs: {len(msa_ids)}")
    print(f"   Data Vintage: {data_vintage}")
    print(f"   Batch Size: {batch_size}")
    print(f"   Max Workers: {max_workers}")
    print()

    # Split MSAs into batches
    batches = [msa_ids[i:i + batch_size] for i in range(0, len(msa_ids), batch_size)]

    results = {
        "batches": len(batches),
        "batch_results": [],
        "total_processed": 0,
        "total_failed": 0
    }

    for i, batch in enumerate(batches):
        print(f"üîÑ Processing batch {i+1}/{len(batches)} ({len(batch)} MSAs)...")

        try:
            batch_result = run_supply_etl_pipeline(
                batch,
                validate_results=True,
                dry_run=False
            )
            results["batch_results"].append(batch_result)
            results["total_processed"] += batch_result.get("msas_processed", 0)
            results["total_failed"] += batch_result.get("msas_failed", 0)
        except Exception as e:
            print(f"‚ùå Batch {i+1} failed: {e}")
            results["batch_results"].append({"error": str(e)})
            results["total_failed"] += len(batch)

    print("
üèÅ Production pipeline completed!"    print(f"   Processed: {results['total_processed']} MSAs")
    print(f"   Failed: {results['total_failed']} MSAs")
    print(f"   Success Rate: {(results['total_processed'] / len(msa_ids) * 100)".1f"}%")

    return results


def main():
    """Main CLI function."""
    parser = argparse.ArgumentParser(description="Run Supply Constraint ETL Pipeline")
    parser.add_argument("--msa-ids", nargs="+", default=["MSA001", "MSA002", "MSA003"],
                       help="MSA identifiers to process")
    parser.add_argument("--start-year", type=int, default=2010,
                       help="Start year for data collection")
    parser.add_argument("--end-year", type=int,
                       help="End year for data collection (default: current year)")
    parser.add_argument("--data-vintage", default=None,
                       help="Data vintage string (YYYY-MM-DD)")
    parser.add_argument("--batch-size", type=int, default=10,
                       help="Batch size for processing")
    parser.add_argument("--max-workers", type=int, default=4,
                       help="Maximum parallel workers")
    parser.add_argument("--production", action="store_true",
                       help="Run in production mode with parallel processing")
    parser.add_argument("--dry-run", action="store_true",
                       help="Run in dry-run mode (no database writes)")
    parser.add_argument("--validate", action="store_true", default=True,
                       help="Run validation after processing")
    parser.add_argument("--log-level", default="INFO",
                       help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    parser.add_argument("--log-file",
                       help="Log file path")

    args = parser.parse_args()

    # Set up logging
    setup_logging(args.log_level, args.log_file)

    # Determine data vintage
    if args.data_vintage:
        data_vintage = args.data_vintage
    else:
        data_vintage = datetime.now().strftime("%Y-%m-%d")

    print("Aker Supply Constraint ETL Pipeline")
    print("=" * 50)

    try:
        if args.production:
            # Run production pipeline
            results = run_production_etl_pipeline(
                args.msa_ids,
                data_vintage,
                args.batch_size,
                args.max_workers
            )
        else:
            # Run standard pipeline
            results = run_supply_etl_pipeline(
                args.msa_ids,
                args.start_year,
                args.end_year,
                args.validate,
                args.dry_run
            )

        print("
üéâ Pipeline execution completed!"        print(f"   Status: {'Success' if 'error' not in results else 'Failed'}")

        if 'error' not in results:
            print(f"   MSAs Processed: {results.get('msas_processed', 0)}")
            print(f"   Data Vintage: {results.get('data_vintage', 'N/A')}")

        return results

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Pipeline interrupted by user")
        return {"status": "interrupted"}
    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    main()
